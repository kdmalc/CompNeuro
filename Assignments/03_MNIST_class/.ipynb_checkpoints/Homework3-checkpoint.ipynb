{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Homework 3\n",
    "##\n",
    "## simple MNIST classifier network\n",
    "##\n",
    "## NSC3270/5270 Spring 2019\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# supress some unnecessary warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load, display, and format mnist images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# load mnist images\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "\n",
      "test\n",
      "(10000, 28, 28)\n",
      "(10000,)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# check dimensions and type of images and labels\n",
    "\n",
    "print('train')\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(type(train_images))\n",
    "print(type(train_labels))\n",
    "print()\n",
    "\n",
    "print('test')\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)\n",
    "print(type(test_images))\n",
    "print(type(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60,000 train images and 10,000 test images. Each image is 28x28 pixels (gray scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the first 9 digits\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(train_images[i], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Digit: {}\".format(train_labels[i]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reformat training and testing images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images shape:  (60000, 28, 28)\n",
      "Training pixel type:    <class 'numpy.uint8'>\n",
      "Testing images shape:   (10000, 28, 28)\n",
      "Testing pixel type:     <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "# check out (again) dimensions and types of mnist data\n",
    "print('Training images shape: ', train_images.shape)\n",
    "print('Training pixel type:   ', type(train_images[0][0][0]))\n",
    "print('Testing images shape:  ', test_images.shape)\n",
    "print('Testing pixel type:    ', type(test_images[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training images shape:  (60000, 784)\n",
      "New training pixel type:    <class 'numpy.float32'>\n",
      "New testing images shape:   (10000, 784)\n",
      "New testing pixel type:     <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# number of images\n",
    "Ntr = train_images.shape[0]\n",
    "Nts = test_images.shape[0]\n",
    "\n",
    "# image shape\n",
    "szx = train_images.shape[1]\n",
    "szy = train_images.shape[2]\n",
    "\n",
    "# need to reshape the 28x28 training/testing images as vectors\n",
    "train_images_vec = train_images.reshape((Ntr, szx * szy))\n",
    "test_images_vec = test_images.reshape(  (Nts, szx * szy))\n",
    "\n",
    "# deciding to normalize the pixels to 0..1 and recase as float32\n",
    "train_images_vec = train_images_vec.astype('float32') / 255\n",
    "test_images_vec = test_images_vec.astype('float32') / 255\n",
    "\n",
    "# display new input dimensions/type\n",
    "print('New training images shape: ', train_images_vec.shape)\n",
    "print('New training pixel type:   ', type(train_images_vec[0][0]))\n",
    "print('New testing images shape:  ', test_images_vec.shape)\n",
    "print('New testing pixel type:    ', type(test_images_vec[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reformat training and testing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels shape:  (60000,)\n",
      "Training labels type:   <class 'numpy.uint8'>\n",
      "\n",
      "First 9 training labels as labels:\n",
      " [5 0 4 1 9 2 1 3 1]\n"
     ]
    }
   ],
   "source": [
    "# check out dimensions and types of mnist data\n",
    "print('Training labels shape: ', train_labels.shape)\n",
    "print('Training labels type:  ', type(train_labels[0]))\n",
    "print()\n",
    "\n",
    "# check out what the first 9 labels look like\n",
    "print(\"First 9 training labels as labels:\\n\", train_labels[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 9 training labels as one-hot encoded vectors:\n",
      " [[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# also need to categorically encode the labels as \"one hot\"\n",
    "\n",
    "train_labels_onehot = to_categorical(train_labels)\n",
    "test_labels_onehot = to_categorical(test_labels)\n",
    "\n",
    "print(\"First 9 training labels as one-hot encoded vectors:\\n\", train_labels_onehot[:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training labels shape (one hot):  (60000, 10)\n",
      "New training labels type (one hot):   <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# display new output dimensions/type\n",
    "print('New training labels shape (one hot): ', train_labels_onehot.shape)\n",
    "print('New training labels type (one hot):  ', type(train_labels_onehot[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define and train neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "\n",
      "layer name : dense | input shape : (?, 784) | output shape : (?, 10)\n",
      "\n",
      "{'name': 'dense', 'trainable': True, 'batch_input_shape': (None, 784), 'dtype': 'float32', 'units': 10, 'activation': 'sigmoid', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None, 'dtype': 'float32'}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {'dtype': 'float32'}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import tools for basic keras networks \n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "nout = 10\n",
    "\n",
    "# create architecture of simple neural network model\n",
    "# input layer  : 28*28 = 784 input nodes\n",
    "# output layer : 10 (nout) output nodes\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(nout, \n",
    "                         activation='sigmoid', \n",
    "                         input_shape=(szx * szy,)))\n",
    "\n",
    "# print a model summary\n",
    "print(network.summary())\n",
    "print()\n",
    "for layer in network.layers:\n",
    "    print('layer name : {} | input shape : {} | output shape : {}'.format(layer.name, layer.input.shape, layer.output.shape))\n",
    "print()\n",
    "for layer in network.layers:\n",
    "    print(layer.get_config())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 2s 32us/sample - loss: 0.1508 - acc: 0.1358 - val_loss: 0.1072 - val_acc: 0.1970\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0985 - acc: 0.2541 - val_loss: 0.0930 - val_acc: 0.3132\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 1s 28us/sample - loss: 0.0903 - acc: 0.3698 - val_loss: 0.0875 - val_acc: 0.4325\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 1s 28us/sample - loss: 0.0860 - acc: 0.4578 - val_loss: 0.0837 - val_acc: 0.5012\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.0827 - acc: 0.5078 - val_loss: 0.0804 - val_acc: 0.5447\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0797 - acc: 0.5383 - val_loss: 0.0774 - val_acc: 0.5738\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0770 - acc: 0.5592 - val_loss: 0.0746 - val_acc: 0.5952\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0744 - acc: 0.5781 - val_loss: 0.0720 - val_acc: 0.6177\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0721 - acc: 0.5944 - val_loss: 0.0695 - val_acc: 0.6360\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 1s 28us/sample - loss: 0.0699 - acc: 0.6098 - val_loss: 0.0673 - val_acc: 0.6503\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0679 - acc: 0.6239 - val_loss: 0.0653 - val_acc: 0.6617\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0661 - acc: 0.6367 - val_loss: 0.0634 - val_acc: 0.6752\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.0645 - acc: 0.6482 - val_loss: 0.0617 - val_acc: 0.6860\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0629 - acc: 0.6571 - val_loss: 0.0601 - val_acc: 0.6945\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0615 - acc: 0.6658 - val_loss: 0.0587 - val_acc: 0.7010\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 2s 28us/sample - loss: 0.0602 - acc: 0.6735 - val_loss: 0.0574 - val_acc: 0.7087\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0591 - acc: 0.6811 - val_loss: 0.0562 - val_acc: 0.7153\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 2s 30us/sample - loss: 0.0580 - acc: 0.6890 - val_loss: 0.0550 - val_acc: 0.7205\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 2s 31us/sample - loss: 0.0569 - acc: 0.6968 - val_loss: 0.0540 - val_acc: 0.7277\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 2s 29us/sample - loss: 0.0560 - acc: 0.7044 - val_loss: 0.0530 - val_acc: 0.7355\n",
      "Done training!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compile network\n",
    "network.compile(optimizer='sgd', \n",
    "                loss='mean_squared_error', \n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# now train the network\n",
    "history = network.fit(train_images_vec, \n",
    "                      train_labels_onehot, \n",
    "                      verbose=True, \n",
    "                      validation_split=.1, \n",
    "                      epochs=20, \n",
    "                      batch_size=128)\n",
    "print('Done training!')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 41us/sample - loss: 0.0544 - acc: 0.7191\n",
      "test_acc: 0.7191\n"
     ]
    }
   ],
   "source": [
    "# test network\n",
    "test_loss, test_acc = network.evaluate(test_images_vec, \n",
    "                                       test_labels_onehot, \n",
    "                                       verbose=True)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some pieces needed to complete Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W (784, 10) | B (10,)\n"
     ]
    }
   ],
   "source": [
    "# get learned network weights and biases\n",
    "\n",
    "W = network.layers[0].get_weights()[0]     # weights input to hidden\n",
    "B = network.layers[0].get_weights()[1]     # bias to hidden\n",
    "\n",
    "print('W {} | B {}'.format(W.shape, B.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(1, 784)\n"
     ]
    }
   ],
   "source": [
    "# model predictions (all 10000 test images)\n",
    "out = network.predict(test_images_vec)\n",
    "\n",
    "# model predictions (a single test image)\n",
    "example = test_images_vec[123]\n",
    "print(example.shape)\n",
    "\n",
    "# vector passed to network.predict must be (?, 784)\n",
    "example = example.reshape((1,example.shape[0]))\n",
    "print(example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "##\n",
    "## Homework 3 Solution Area\n",
    "##\n",
    "\n",
    "## Q1. The original MNIST test_labels numpy array contains the digit value associated\n",
    "## with the corresponding digit image (test_images). The output from the network (from\n",
    "## out = network.predict(test_images_vec) above) contains the activations of the 10\n",
    "## output nodes for every test image presented to the network. Write a function that\n",
    "## takes the (10000,10) numpy array of output (of type float) activations and returns a \n",
    "## (10000,) numpy array of discrete digit classification by the network (of type int).\n",
    "## Specifically, create a test_decisions numpy array of the same size and type as the\n",
    "## MNIST test_labels array you started with. Whereas test_labels shows the correct\n",
    "## answer, test_decisions shows the ultimate decision by the network. Below you will use \n",
    "## both arrays to pull out test images that the network classifies correctly vs. incorrectly.\n",
    "##\n",
    "## To turn a numpy array of continuous output activations into a discrete digit classification,\n",
    "## just take the maximum output as the \"winner\" that \"takes all\", determining the classification.\n",
    "##\n",
    "## In your function, feel free to use for loops. Here, we are looking to see that you understand\n",
    "## how to use the outputs generated by the network, not whether you can program using the\n",
    "## most efficient Python style.\n",
    "\n",
    "### INSERT Q1 SOLUTION HERE ###\n",
    "\n",
    "out_max = np.amax(out,axis=1,keepdims=False)\n",
    "\n",
    "test_decisions = np.zeros((len(out_max),1))\n",
    "for idx in range(len(out_max)):\n",
    "    # Did not convert to int ...........................................................................\n",
    "    test_decisions[idx] = int(np.where(out[idx,:] == out_max[idx])[0][0])\n",
    "\n",
    "# test_labels.shape = (10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decisions[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple of arrays returned :  (array([0, 2], dtype=int64), array([0, 2], dtype=int64))\n",
      "List of coordinates of minimum value in Numpy array : \n",
      "(0, 0)\n",
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# Beta test code\n",
    "\n",
    "arr2D = np.array([[11, 12, 13],\n",
    "                     [14, 15, 16],\n",
    "                     [17, 15, 11],\n",
    "                     [12, 14, 15]])\n",
    "\n",
    "# Find index of minimum value from 2D numpy array\n",
    "result = np.where(arr2D == np.amin(arr2D))\n",
    "print('Tuple of arrays returned : ', result)\n",
    "print('List of coordinates of minimum value in Numpy array : ')\n",
    "# zip the 2 arrays to get the exact coordinates\n",
    "listOfCordinates = list(zip(result[0], result[1]))\n",
    "# travese over the list of cordinates\n",
    "for cord in listOfCordinates:\n",
    "    print(cord)\n",
    "    \n",
    "###############################################################\n",
    "\n",
    "# test case:\n",
    "me = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "us = np.amax(me,axis=1,keepdims=False)\n",
    "print(us)\n",
    "\n",
    "for idx in range(len(us)):\n",
    "    col = np.where(me[idx,:] == us[idx])[0][0]\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##\n",
    "## Q2. Comparing the correct answers (test_labels) and network classifications (test_decisions),\n",
    "## for each digit 0..9, find one test image (test_image) that is classified by the network\n",
    "## correctly and one test image that is classified by the network incorrectly. \n",
    "##\n",
    "## Create a 2x10 plot of digit images (feel free to adapt the code above that uses subplot), with a \n",
    "## column for each digit 0..9 with the first row showing examples correctly classified (one example \n",
    "## for each digit) and the second row showing the examples incorrectly classified (one example \n",
    "## for each digit). Each subplot title should show the answer and the classification response \n",
    "## (e.g., displaying 4/2 as the title, if the correct answer is 4 and the classification was 2).\n",
    "##\n",
    "\n",
    "### INSERT Q2 SOLUTION HERE ###\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(20):\n",
    "    plt.subplot(2,10,i+1)\n",
    "    #plt.tight_layout() #Turning this off shows all 20, but the spacing is cancer with it off\n",
    "    plt.imshow(train_images[0], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Digit: {}\".format(train_labels[0]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True]\n",
      " [False  True]]\n",
      "(array([0, 1], dtype=int64), array([1, 1], dtype=int64))\n",
      "[[0 1]\n",
      " [0 1]]\n",
      "test_labels: <class 'numpy.ndarray'>, (10000,)\n",
      "test_decisions: <class 'numpy.ndarray'>, (10000, 1)\n",
      "test_labelsRE: <class 'numpy.ndarray'>, (10000, 1)\n",
      "test_decisionsRE: <class 'numpy.ndarray'>, (10000, 1)\n",
      "-----------------------------------------\n",
      "(10000, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# This is not the vector that he wants\n",
    "#ValueError: could not broadcast input array from shape (10000,1) into shape (10000,)\n",
    "\n",
    "a= np.array([[1,2],[3,4]])\n",
    "b= np.array([[3,2],[1,4]])\n",
    "\n",
    "#1) find out which values are the same\n",
    "print(a==b)\n",
    "# array([[False,  True],\n",
    "#        [False,  True]])\n",
    "\n",
    "#2) get the index of the same values?\n",
    "print(np.where((a==b) == True)) # or np.where(a==b)\n",
    "#(array([0, 1]), array([1, 1]))\n",
    "\n",
    "# Adding on to the previous question, is there a way for me to return 1 if the values are the same and 0 otherwise\n",
    "print((a==b).astype(int))\n",
    "# array([[0, 1],\n",
    "#        [0, 1]])\n",
    "print(\"-----------------------------------------\")\n",
    "###########################################################################################\n",
    "\n",
    "test_labelsRE = np.reshape(test_labels, (10_000, 1))\n",
    "test_decisionsRE = np.reshape(test_decisions, (10_000, 1))\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"test_labels: {type(test_labels)}, {test_labels.shape}\")\n",
    "print(f\"test_decisions: {type(test_decisions)}, {test_decisions.shape}\")\n",
    "print(\"-----------------------------------------\")\n",
    "print(f\"test_labelsRE: {type(test_labelsRE)}, {test_labelsRE.shape}\")\n",
    "print(f\"test_decisionsRE: {type(test_decisionsRE)}, {test_decisionsRE.shape}\")\n",
    "print(\"-----------------------------------------\")\n",
    "#1) find out which values are the same\n",
    "res_comp = test_labelsRE==test_decisionsRE\n",
    "\n",
    "#2) get the index of the same values?\n",
    "np.where((res_comp) == True) # or np.where(a==b)\n",
    "\n",
    "# Adding on to the previous question, is there a way for me to return 1 if the values are the same and 0 otherwise\n",
    "res_comp_bool = (res_comp).astype(int)\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "print(res_comp.shape)\n",
    "print(res_comp_bool.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 4 1 4 9 5 9]\n",
      "[2. 1. 0. 4. 1. 4. 9. 0. 7.]\n",
      "[ True  True  True  True  True  True  True False False]\n"
     ]
    }
   ],
   "source": [
    "print(test_labelsRE[1:10,0])\n",
    "print(test_decisionsRE[1:10,0])\n",
    "print(res_comp[1:10,0])\n",
    "\n",
    "true_dict = {x: -1 for x in list(range(10))}\n",
    "false_dict = {x: -1 for x in list(range(10))}\n",
    "\n",
    "true_count = 0\n",
    "false_count = 0\n",
    "while false_count+true_count<20:\n",
    "    for idx in range(len(res_comp)):\n",
    "        if res_comp[idx]==True and true_count<10:\n",
    "            pass\n",
    "            if true_dict[test_labelsRE[idx]]==-1:\n",
    "                pass\n",
    "        elif res_comp[idx]==False and false_count<10:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Q3. Create \"images\" of the connection weight adapting the code used to display\n",
    "## the actual digit images. There should be 10 weight images, an image for each\n",
    "## set of weight connecting the input layer (784 inputs) to each output node.\n",
    "## You will want to reshape the (784,1) vector of weights to a (28,28) image and\n",
    "## display the result using imshow()\n",
    "\n",
    "### INSERT Q3 SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Q4. Use the weight matrix (W), bias vector (B), and activation function (simple sigmoid)\n",
    "## to reproduce in your own code the outputs (out) generated by the network (from\n",
    "## this out = network.predict(test_images_vec))\n",
    "##\n",
    "## The simple sigmoid activation function is defined as follows:\n",
    "## f(x) = 1 / (1+exp(-x))\n",
    "##\n",
    "## Confirm that your output vectors and the keras-produced output vectors are the same\n",
    "## (within some small epsilon since floating point calculations will often not come out\n",
    "## exactly the same on computers).\n",
    "##\n",
    "\n",
    "### INSERT Q4 SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
