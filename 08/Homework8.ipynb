{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874620ef-c649-4f3c-8d28-3a200414e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    from tensorflow.keras import models\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras import optimizers\n",
    "    \n",
    "    from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084e015",
   "metadata": {},
   "source": [
    "https://brightspace.vanderbilt.edu/content/enforced/320206-2021F.1.NSC.3270.01/Week13a.pdf\n",
    "\n",
    "https://brightspace.vanderbilt.edu/d2l/le/content/320206/viewContent/2121494/View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941cc3b-0b9f-4bbd-bc0b-f52a994622e6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### read in and prepare Haxby data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c4507e-f2af-43b2-aca6-58ab0740e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Haxby data (see slides on Brightspace for details)\n",
    "\n",
    "def prep_haxby_data(dirpath = ''):\n",
    "    ### load the functional MRI patterns ###\n",
    "\n",
    "    # every 2.5 seconds the scanner records a full brain image\n",
    "    # we have extracted just voxels in ventral temporal lobe\n",
    "    # row number corresponds to image number / time point\n",
    "    # column number corresponds to voxel number within ventral temporal lobe\n",
    "    # there should be 1452 images, and 32450 voxels\n",
    "    full_patterns = np.load(dirpath + 'haxby_vt_patterns.npy')\n",
    "    n_img = full_patterns.shape[0]\n",
    "    n_vox = full_patterns.shape[1]\n",
    "\n",
    "    # if an fMRI experiment takes an hour, you usually wouldn't run \n",
    "    # the scanner for an hour straight\n",
    "    # usually, you would run it for a block of time (several minutes) \n",
    "    # called a 'run'\n",
    "    # each run contains a chunk of the experiment\n",
    "    # if there are several conditions in your experiment, usually you \n",
    "    # try to put each condition within each run\n",
    "    # Haxby et al had 8 experimental conditions, one condition for each \n",
    "    # of the 8 categories they used - they had 12 runs, and within each \n",
    "    # run they presented a block of images from the each category.\n",
    "    n_runs = 12\n",
    "\n",
    "    ### processing the experimental design ###\n",
    "    \n",
    "    # Haxby et al. provide a text file 'labels.txt' which says when \n",
    "    # they presented items from one category or another\n",
    "    # labels.txt has 1453 lines.  \n",
    "    # the first line is header information  \n",
    "    # then there are 1452 lines, one for each image, saying what was \n",
    "    # happening in the experiment at that time point\n",
    "    # each line specifies the category name / condition, and the run \n",
    "    # number (though the header calls this \"chunks\")\n",
    "    # aside from the 8 categories, there is also time when the screen was \n",
    "    # blank, in between category presentations\n",
    "    # they call this 'rest'; we will exclude rest images from our classification\n",
    "\n",
    "    # a bunch of items from the same category would be presented one \n",
    "    # after another\n",
    "    # so if you look at labels.txt, you'll see there will be 9 lines in a \n",
    "    # row that all say 'scissors'\n",
    "\n",
    "    # read the labels.txt file\n",
    "    fid = open(dirpath + 'labels.txt', 'r')\n",
    "    # this command reads in the first line of the file, the header, \n",
    "    # which we don't need \n",
    "    temp = fid.readline()\n",
    "\n",
    "    # these are the 9 different strings that appear in labels.txt\n",
    "    cond_names = ['face', 'house', 'cat', 'shoe', 'scissors', 'bottle', \n",
    "                  'scrambledpix', 'chair', 'rest']\n",
    "    # this will store the 'one-hot' target values for the classifier\n",
    "    all_targets = np.zeros([n_img, len(cond_names)])\n",
    "    # this will store the run index\n",
    "    all_runs = np.zeros([n_img,])\n",
    "\n",
    "    # now we iterate through the 1452 lines in labels.txt\n",
    "    # line.split breaks the string into 2 parts\n",
    "    # the first is 'this_cond' which says which category/condition (or rest)\n",
    "    # the second is 'this_run' which tells you the functional run of this image \n",
    "    imgcount = 0\n",
    "    for line in fid:\n",
    "        temp = line.split()\n",
    "        this_cond = temp[0]\n",
    "        this_run = temp[1]\n",
    "        all_targets[imgcount, cond_names.index(this_cond)] = 1\n",
    "        all_runs[imgcount] = int(this_run)\n",
    "        imgcount += 1\n",
    "\n",
    "    ### cheap version of accounting for the hemodynamic lag ###\n",
    "\n",
    "    # as we reviewed in lecture, there is a lag from when a stimulus is \n",
    "    # presented, to when the brain's vasculature has its peak response\n",
    "    # one can properly take this hemodynamic lag into account, but we \n",
    "    # are going to do a short-cut\n",
    "    # if each image takes 2.5 seconds to acquire, and it takes about 5 \n",
    "    # seconds for the brain to reach its peak response, we can shift the \n",
    "    # condition labels forward by two images\n",
    "    # an easy way to do this is to concatenate 2 rows of zeros at the \n",
    "    # beginning of the targets matrix and then remove 2 rows of zeros \n",
    "    # from the end of the targets matrix\n",
    "    # that pushes every label forward by 5 seconds\n",
    "    \n",
    "    # just shift all the regressors over by 2 time points\n",
    "    # each image takes 2.5 seconds to acquire\n",
    "    prepend_zeros = np.zeros([2, all_targets.shape[1]])\n",
    "    shift_all_targets = np.concatenate((prepend_zeros, all_targets))\n",
    "    mask = np.ones(shift_all_targets.shape[0], dtype=bool)\n",
    "    mask[shift_all_targets.shape[0]-2:shift_all_targets.shape[0]] = False\n",
    "    shift_all_targets = shift_all_targets[mask, :]\n",
    "    \n",
    "    # now that we are done just replace the original target labels \n",
    "    # with the shifted ones\n",
    "    all_targets = shift_all_targets\n",
    "\n",
    "    ### remove rests from runs ###\n",
    "    \n",
    "    # modify the labels matrix\n",
    "    # remove the rest unit, we don't want to classify rest patterns\n",
    "    temp_targets = all_targets[:, :8].copy()\n",
    "    # get rid of images where there isn't a category being presented\n",
    "    # and make it a boolean array\n",
    "    label_present = np.sum(temp_targets, axis=1)> 0\n",
    "\n",
    "    # label_present is a boolean array indicating when a category is \n",
    "    # being presented\n",
    "    # we are using it as a mask to get rid of all the time-points \n",
    "    # where nothing is being presented\n",
    "    # in other words it gets rid of all the rest periods\n",
    "    targets = temp_targets[label_present, :]\n",
    "    patterns = full_patterns[label_present, :]\n",
    "    runs = all_runs[label_present]\n",
    "    \n",
    "    ### z-scoring is a kind of normalization ###\n",
    "    n_vox = patterns.shape[1]\n",
    "    for i in range(n_vox):\n",
    "        patterns[:,i] = (patterns[:,i] - patterns[:,i].mean()) / patterns[:,i].std()\n",
    "\n",
    "    # for a set of numbers, get the mean and standard deviation\n",
    "    # subtract the mean off every number, divide every number \n",
    "    # by the standard deviation\n",
    "    # here we normalize our patterns using a temporal z-score,\n",
    "    # meaning that we normalize the values for each voxel, across time    \n",
    "    \n",
    "    # here are the arrays you need to do classification\n",
    "    return patterns, targets, runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5a472-286d-4c22-b1f4-04d7b48bab5b",
   "metadata": {},
   "source": [
    "patterns.shape, targets.shape, runs.shape<br>\n",
    "864 : number of fMRI brain scans (onebrain scan per object image)<br>\n",
    "32450 : number voxels in IT cortex (using an anatomical mask)<br>\n",
    "8 : number of object categories (face, house, cat, shoe, scissors, bottle, scrambled, chair, rest)\n",
    "\n",
    "patterns: contains voxel (fMRI) data\n",
    "\n",
    "targets: contains the condition (category) associated with each fMRI scan\n",
    "\n",
    "runs: which \"run\" each scan is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60283587-270d-4431-8159-ce5e6d64935e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 32450)\n",
      "(864, 8)\n",
      "(864,)\n"
     ]
    }
   ],
   "source": [
    "# load Haxby et al. data (see slides on Brightspace)\n",
    "\n",
    "patterns, targets, runs = prep_haxby_data()\n",
    "\n",
    "print(patterns.shape)\n",
    "print(targets.shape)\n",
    "print(runs.shape)\n",
    "\n",
    "cond_names = ['face', 'house', 'cat', 'shoe', 'scissors', 'bottle',\n",
    "              'scrambledpix', 'chair', 'rest']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a8be1-ab6a-4a66-b781-7a0f6f86fabe",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### \"feature selection\"\n",
    "\n",
    "with \"feature selection\", selecting voxels that modulate statistically significantly according to object category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec0f3e5-8b6f-4605-b387-1c1509d0838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"feature selection\" based on picking statistically significant voxels\n",
    "\n",
    "def feature_selection(train_pats, train_targs, test_pats):\n",
    "    pval = 0.1\n",
    "    \n",
    "    keep_these = np.zeros((train_pats.shape[1],))\n",
    "    \n",
    "    # loop through every voxel\n",
    "    for v in range(train_pats.shape[1]):\n",
    "        groups = []\n",
    "        # Loop through all 8 categories\n",
    "        for c in range(train_targets.shape[1]):\n",
    "            groups.append(train_pats[train_targs[:, c] == 1., v])\n",
    "           \n",
    "        # and statistically analyze it for category modulation\n",
    "        temp = sps.f_oneway(groups[0], groups[1], groups[2], groups[3],\n",
    "                            groups[4], groups[5], groups[6], groups[7])\n",
    "\n",
    "        if temp.pvalue < pval:\n",
    "            keep_these[v] = 1.\n",
    "    \n",
    "    keep_these = keep_these.astype(bool)\n",
    "    train_pats = train_pats[:, keep_these]\n",
    "    test_pats = test_pats[:, keep_these]\n",
    "    \n",
    "    return train_pats, test_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2fce4a-41af-4385-a528-9cec3726cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12763\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# an example \"leaving out\" a particular run\n",
    "example_test_run = 1\n",
    "\n",
    "train_these = runs != example_test_run\n",
    "test_these  = runs == example_test_run\n",
    "\n",
    "# an example of logical indexing\n",
    "train_patterns = patterns[train_these, :]\n",
    "train_targets = targets[train_these, :]\n",
    "\n",
    "test_patterns = patterns[test_these, :]\n",
    "test_targets = targets[test_these, :]\n",
    "\n",
    "# \"feature selection\" of statistically category-modulated voxels\n",
    "\n",
    "fs_train_patterns, fs_test_patterns = feature_selection(train_patterns,\n",
    "                                                        train_targets,\n",
    "                                                        test_patterns)\n",
    "\n",
    "# report some metrics on Haxby experiment (n_vox is for this fold)\n",
    "n_runs = np.max(runs).astype('int') + 1\n",
    "n_vox  = fs_train_patterns.shape[1]\n",
    "n_cats = targets.shape[1]\n",
    "\n",
    "cond_names = ['face', 'house', 'cat', 'shoe', 'scissors', 'bottle', \n",
    "              'scrambledpix', 'chair', 'rest']\n",
    "\n",
    "print(n_runs)\n",
    "print(n_vox)\n",
    "print(n_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae2c30",
   "metadata": {},
   "source": [
    "# Q1a:\n",
    "> Leave-one-out cross validation\n",
    "\n",
    "Save the classification\tfor\teach of\tthe\t12 folds and note the classification performance on\teach of\t the 8\tcategories and\tfor\teach of\tthe\t5 iterations. First, report\tthe\taverage\tclassification accuracy overall. Next, examine and\treport\tthe\taverage\tclassification\tfor\teach object category. Are certain\tcategories classified more accurately?\tHow\tmuch variability is\tthere across categories\tand\tacross\tfolds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b57decde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP THE FOLD R         \n",
    "# DO FEATURE SELECTION FOR THIS FOLD\n",
    "def cross_fold_fs(example_test_run, train_targets=0):\n",
    "    train_these = runs != example_test_run\n",
    "    test_these  = runs == example_test_run\n",
    "\n",
    "    # an example of logical indexing\n",
    "    train_patterns = patterns[train_these, :]\n",
    "    if train_targets == 0:\n",
    "        train_targets = targets[train_these, :]\n",
    "\n",
    "    test_patterns = patterns[test_these, :]\n",
    "    test_targets = targets[test_these, :]\n",
    "\n",
    "    # \"feature selection\" of statistically category-modulated voxels\n",
    "    fs_train_patterns, fs_test_patterns = feature_selection(train_patterns,\n",
    "                                                            train_targets,\n",
    "                                                            test_patterns)\n",
    "    \n",
    "    return fs_train_patterns, fs_test_patterns, test_targets\n",
    "\n",
    "# TRAIN LINEAR CLASSIFIER\n",
    "def make_train_model(nin, nout, fs_train_patterns, train_targets, my_epochs=30, my_batch=128):\n",
    "    network_softmax = models.Sequential()\n",
    "    network_softmax.add(layers.Dense(nout, \n",
    "                             activation='softmax',\n",
    "                             kernel_regularizer=l2(0.001),\n",
    "                             input_shape=(nin,)))\n",
    "\n",
    "    network_softmax.compile(optimizer='adam', \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy', 'mse'])\n",
    "\n",
    "    history = network_softmax.fit(fs_train_patterns,\n",
    "                                  train_targets,\n",
    "                                  epochs=my_epochs,\n",
    "                                  batch_size=my_batch,\n",
    "                                  verbose=False)\n",
    "    \n",
    "    return network_softmax\n",
    "\n",
    "# TEST LINEAR CLASSIFIER\n",
    "def test_classifier(fs_test_patterns, n_cats, idx_cat, test_targets, network_softmax):\n",
    "    # Get train, test arrays for each CATEGORY\n",
    "    base = np.zeros(n_cats)\n",
    "    base[idx_cat] = 1\n",
    "    CAT_test_patterns = find_rows(test_targets, base)\n",
    "    full_base = np.zeros((CAT_test_patterns.shape[0], n_cats))\n",
    "    full_base[:, idx_cat] = 1\n",
    "\n",
    "    # Now, evaluate based on those CATEGORIES\n",
    "    results = network_softmax.evaluate(fs_test_patterns[CAT_test_patterns, :], \n",
    "                           full_base, \n",
    "                           verbose=False)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0dc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller helper functions\n",
    "def plot_model_accuracy(history):\n",
    "    \"\"\"Accuracy Plotting Function\"\"\"\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title('Model Accuracy As a Function of Number of Epochs Trained')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_loss(history):\n",
    "    \"\"\"Loss Plotting Function\"\"\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss As a Function of Number of Epochs Trained')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def find_rows(source, target):\n",
    "    return np.where((source == target).all(axis=1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7df9fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MVPA_crossfolds(n_runs=5, n_cross_folds=12, n_cats=targets.shape[1], train_targets=0):\n",
    "    nout = n_cats\n",
    "    test_patterns_array = [-1]*n_cats\n",
    "    test_targets_array = [-1]*n_cats\n",
    "    classification_array = np.zeros((n_runs, n_cross_folds, n_cats))\n",
    "    \n",
    "    for run_num in range(n_runs):\n",
    "        for n_cross in range(n_cross_folds):\n",
    "            # SET UP THE FOLD R, DO FEATURE SELECTION FOR THIS FOLD\n",
    "            fs_train_patterns, fs_test_patterns, test_targets = cross_fold_fs(n_cross, train_targets=0)\n",
    "\n",
    "            # Input dimensions is the number of voxels\n",
    "            nin  = fs_train_patterns.shape[1]\n",
    "\n",
    "            # TRAIN LINEAR CLASSIFIER\n",
    "            network_softmax = make_train_model(nin, nout, fs_train_patterns, train_targets)\n",
    "\n",
    "            for idx_cat in range(n_cats):\n",
    "                # TEST LINEAR CLASSIFIER\n",
    "                results = test_classifier(fs_test_patterns, n_cats, idx_cat, test_targets, network_softmax)\n",
    "\n",
    "                # SAVE CLASSIFIER PERFORMANCE\n",
    "                classification_array[run_num, n_cross, idx_cat] = results[1]\n",
    "\n",
    "    # REPORT OVERALL CLASSIFIER ACCURACY\n",
    "    oaca = np.sum(classification_array)/classification_array.size\n",
    "    print(f\"The overall average classification accuracy is {oaca:.4}%\")\n",
    "    # SAVE AVERAGE PERFORMANCE TO ARRAY TO CALCUALTE CHANCE RANGE\n",
    "    cat_accs = np.zeros((n_runs, n_cats))\n",
    "\n",
    "    for i in range(n_runs):\n",
    "        for k in range(n_cats):\n",
    "            old_sum = cat_accs[i, k]\n",
    "            new_sum = np.sum(classification_array[i, :, k])\n",
    "            running_sum = old_sum + new_sum\n",
    "            cat_accs[i, k] = running_sum\n",
    "\n",
    "    cat_accs_list = []\n",
    "\n",
    "    for i in range(n_cats):\n",
    "        cat_accs_list.append(np.sum(cat_accs[:, i]))\n",
    "\n",
    "    cat_accs_final = [acc/(classification_array.size/n_cats) for acc in cat_accs_list]\n",
    "    \n",
    "    return cat_accs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de1032",
   "metadata": {},
   "source": [
    "Are\tcertain\tcategories classified more accurately? How\tmuch variability is there across categories\tand\tacross\tfolds?\n",
    "\n",
    "> Clearly, there is great variability across the different categories, with some reaching accuracies of nearly 50% whereas others only reach 2%.  Most are centered are 30%, but the average is brought down by the 2% case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf31e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall average classification accuracy is 0.2616%\n"
     ]
    }
   ],
   "source": [
    "cat_accs_final = MVPA_crossfolds(n_runs=5, n_cross_folds=12, n_cats=targets.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fa28e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33888889389733473,\n",
       " 0.4851851945122083,\n",
       " 0.1000000008692344,\n",
       " 0.15185185347994168,\n",
       " 0.29814815024534863,\n",
       " 0.025925926119089126,\n",
       " 0.35740741305053236,\n",
       " 0.3351851916561524]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_accs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977678a-e405-4651-ba68-db4770a1be8b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### function to scramble targets for permutation test\n",
    "\n",
    "(as described in class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5771093",
   "metadata": {},
   "source": [
    "what if we were to scramble the labels associated\n",
    "with different brain scans (don't touch brain scans)?\n",
    "e.g., on one run, voxel measures when a face is shown\n",
    "would be labeled as being trials when a chair was shown,\n",
    "and on another run, voxel measures when a face is shown\n",
    "would be labeled as being trials when a shoe was shown\n",
    "the performance for a classifier trained on that random\n",
    "shuffling of labels would indicate the level of performance\n",
    "you would expect for a classifier \"by chance\"\n",
    "\n",
    "PERMUTATE THE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea2870-1fbd-49d1-9d71-e02e85d728fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to scramble the category labels for a permutation analysis\n",
    "\n",
    "def scramble_targets(targets, runs):\n",
    "    # there are 12 runs\n",
    "    n_runs = np.max(runs).astype('int') + 1\n",
    "    \n",
    "    # there are 8 categories\n",
    "    n_cats = targets.shape[1]\n",
    "    \n",
    "    # this will contain the scrambled category labels\n",
    "    scram_targets = np.zeros(targets.shape)\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        # first find the category labels just for this run\n",
    "        these_targets = targets[runs==i, :].copy()\n",
    "        \n",
    "        # this shuffles the columns of these_targets, which preserves \n",
    "        # the block structure of the experiment\n",
    "        scram_targets_this_run = these_targets[:, np.random.permutation(n_cats)]\n",
    "        \n",
    "        # this copies the scrambled targets for this run into the \n",
    "        # appropriate rows of the scrambled category labels matrix\n",
    "        scram_targets[runs==i,:] = scram_targets_this_run\n",
    "\n",
    "    return scram_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4a6a1-bb8c-4fa1-8779-6f65c09f5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_perms = 10\n",
    "\n",
    "for p in range(n_perms):\n",
    "    # PERMUTE LABELS\n",
    "    scram_targets = scramble_targets(targets, runs)\n",
    "    \n",
    "    cat_accs_final = MVPA_crossfolds(n_runs=5, n_cross_folds=12, n_cats=targets.shape[1], train_targets=scram_train_targets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_accs_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
