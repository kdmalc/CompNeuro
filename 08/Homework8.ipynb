{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "874620ef-c649-4f3c-8d28-3a200414e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.stats as sps\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "    from tensorflow.keras import models\n",
    "    from tensorflow.keras import layers\n",
    "    from tensorflow.keras import optimizers\n",
    "    \n",
    "    from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1084e015",
   "metadata": {},
   "source": [
    "https://brightspace.vanderbilt.edu/content/enforced/320206-2021F.1.NSC.3270.01/Week13a.pdf\n",
    "\n",
    "https://brightspace.vanderbilt.edu/d2l/le/content/320206/viewContent/2121494/View"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941cc3b-0b9f-4bbd-bc0b-f52a994622e6",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### read in and prepare Haxby data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c4507e-f2af-43b2-aca6-58ab0740e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare Haxby data (see slides on Brightspace for details)\n",
    "\n",
    "def prep_haxby_data(dirpath = ''):\n",
    "    ### load the functional MRI patterns ###\n",
    "\n",
    "    # every 2.5 seconds the scanner records a full brain image\n",
    "    # we have extracted just voxels in ventral temporal lobe\n",
    "    # row number corresponds to image number / time point\n",
    "    # column number corresponds to voxel number within ventral temporal lobe\n",
    "    # there should be 1452 images, and 32450 voxels\n",
    "    full_patterns = np.load(dirpath + 'haxby_vt_patterns.npy')\n",
    "    n_img = full_patterns.shape[0]\n",
    "    n_vox = full_patterns.shape[1]\n",
    "\n",
    "    # if an fMRI experiment takes an hour, you usually wouldn't run \n",
    "    # the scanner for an hour straight\n",
    "    # usually, you would run it for a block of time (several minutes) \n",
    "    # called a 'run'\n",
    "    # each run contains a chunk of the experiment\n",
    "    # if there are several conditions in your experiment, usually you \n",
    "    # try to put each condition within each run\n",
    "    # Haxby et al had 8 experimental conditions, one condition for each \n",
    "    # of the 8 categories they used - they had 12 runs, and within each \n",
    "    # run they presented a block of images from the each category.\n",
    "    n_runs = 12\n",
    "\n",
    "    ### processing the experimental design ###\n",
    "    \n",
    "    # Haxby et al. provide a text file 'labels.txt' which says when \n",
    "    # they presented items from one category or another\n",
    "    # labels.txt has 1453 lines.  \n",
    "    # the first line is header information  \n",
    "    # then there are 1452 lines, one for each image, saying what was \n",
    "    # happening in the experiment at that time point\n",
    "    # each line specifies the category name / condition, and the run \n",
    "    # number (though the header calls this \"chunks\")\n",
    "    # aside from the 8 categories, there is also time when the screen was \n",
    "    # blank, in between category presentations\n",
    "    # they call this 'rest'; we will exclude rest images from our classification\n",
    "\n",
    "    # a bunch of items from the same category would be presented one \n",
    "    # after another\n",
    "    # so if you look at labels.txt, you'll see there will be 9 lines in a \n",
    "    # row that all say 'scissors'\n",
    "\n",
    "    # read the labels.txt file\n",
    "    fid = open(dirpath + 'labels.txt', 'r')\n",
    "    # this command reads in the first line of the file, the header, \n",
    "    # which we don't need \n",
    "    temp = fid.readline()\n",
    "\n",
    "    # these are the 9 different strings that appear in labels.txt\n",
    "    cond_names = ['face', 'house', 'cat', 'shoe', 'scissors', 'bottle', \n",
    "                  'scrambledpix', 'chair', 'rest']\n",
    "    # this will store the 'one-hot' target values for the classifier\n",
    "    all_targets = np.zeros([n_img, len(cond_names)])\n",
    "    # this will store the run index\n",
    "    all_runs = np.zeros([n_img,])\n",
    "\n",
    "    # now we iterate through the 1452 lines in labels.txt\n",
    "    # line.split breaks the string into 2 parts\n",
    "    # the first is 'this_cond' which says which category/condition (or rest)\n",
    "    # the second is 'this_run' which tells you the functional run of this image \n",
    "    imgcount = 0\n",
    "    for line in fid:\n",
    "        temp = line.split()\n",
    "        this_cond = temp[0]\n",
    "        this_run = temp[1]\n",
    "        all_targets[imgcount, cond_names.index(this_cond)] = 1\n",
    "        all_runs[imgcount] = int(this_run)\n",
    "        imgcount += 1\n",
    "\n",
    "    ### cheap version of accounting for the hemodynamic lag ###\n",
    "\n",
    "    # as we reviewed in lecture, there is a lag from when a stimulus is \n",
    "    # presented, to when the brain's vasculature has its peak response\n",
    "    # one can properly take this hemodynamic lag into account, but we \n",
    "    # are going to do a short-cut\n",
    "    # if each image takes 2.5 seconds to acquire, and it takes about 5 \n",
    "    # seconds for the brain to reach its peak response, we can shift the \n",
    "    # condition labels forward by two images\n",
    "    # an easy way to do this is to concatenate 2 rows of zeros at the \n",
    "    # beginning of the targets matrix and then remove 2 rows of zeros \n",
    "    # from the end of the targets matrix\n",
    "    # that pushes every label forward by 5 seconds\n",
    "    \n",
    "    # just shift all the regressors over by 2 time points\n",
    "    # each image takes 2.5 seconds to acquire\n",
    "    prepend_zeros = np.zeros([2, all_targets.shape[1]])\n",
    "    shift_all_targets = np.concatenate((prepend_zeros, all_targets))\n",
    "    mask = np.ones(shift_all_targets.shape[0], dtype=bool)\n",
    "    mask[shift_all_targets.shape[0]-2:shift_all_targets.shape[0]] = False\n",
    "    shift_all_targets = shift_all_targets[mask, :]\n",
    "    \n",
    "    # now that we are done just replace the original target labels \n",
    "    # with the shifted ones\n",
    "    all_targets = shift_all_targets\n",
    "\n",
    "    ### remove rests from runs ###\n",
    "    \n",
    "    # modify the labels matrix\n",
    "    # remove the rest unit, we don't want to classify rest patterns\n",
    "    temp_targets = all_targets[:, :8].copy()\n",
    "    # get rid of images where there isn't a category being presented\n",
    "    # and make it a boolean array\n",
    "    label_present = np.sum(temp_targets, axis=1)> 0\n",
    "\n",
    "    # label_present is a boolean array indicating when a category is \n",
    "    # being presented\n",
    "    # we are using it as a mask to get rid of all the time-points \n",
    "    # where nothing is being presented\n",
    "    # in other words it gets rid of all the rest periods\n",
    "    targets = temp_targets[label_present, :]\n",
    "    patterns = full_patterns[label_present, :]\n",
    "    runs = all_runs[label_present]\n",
    "    \n",
    "    ### z-scoring is a kind of normalization ###\n",
    "    n_vox = patterns.shape[1]\n",
    "    for i in range(n_vox):\n",
    "        patterns[:,i] = (patterns[:,i] - patterns[:,i].mean()) / patterns[:,i].std()\n",
    "\n",
    "    # for a set of numbers, get the mean and standard deviation\n",
    "    # subtract the mean off every number, divide every number \n",
    "    # by the standard deviation\n",
    "    # here we normalize our patterns using a temporal z-score,\n",
    "    # meaning that we normalize the values for each voxel, across time    \n",
    "    \n",
    "    # here are the arrays you need to do classification\n",
    "    return patterns, targets, runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e5a472-286d-4c22-b1f4-04d7b48bab5b",
   "metadata": {},
   "source": [
    "patterns.shape, targets.shape, runs.shape<br>\n",
    "864 : number of fMRI brain scans (onebrain scan per object image)<br>\n",
    "32450 : number voxels in IT cortex (using an anatomical mask)<br>\n",
    "8 : number of object categories (face, house, cat, shoe, scissors, bottle, scrambled, chair, rest)\n",
    "\n",
    "patterns: contains voxel (fMRI) data\n",
    "\n",
    "targets: contains the condition (category) associated with each fMRI scan\n",
    "\n",
    "runs: which \"run\" each scan is associated with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60283587-270d-4431-8159-ce5e6d64935e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(864, 32450)\n",
      "(864, 8)\n",
      "(864,)\n"
     ]
    }
   ],
   "source": [
    "# load Haxby et al. data (see slides on Brightspace)\n",
    "\n",
    "patterns, targets, runs = prep_haxby_data()\n",
    "\n",
    "print(patterns.shape)\n",
    "print(targets.shape)\n",
    "print(runs.shape)\n",
    "\n",
    "cond_names = ['face', 'house', 'cat', 'shoe', 'scissors', 'bottle',\n",
    "              'scrambledpix', 'chair', 'rest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12baeafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "        1.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,\n",
       "        4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  4.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,  5.,\n",
       "        5.,  5.,  5.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,  7.,\n",
       "        7.,  7.,  7.,  7.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,\n",
       "        8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  8.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,  9.,\n",
       "        9.,  9.,  9.,  9.,  9., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10.,\n",
       "       10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 10., 11.,\n",
       "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "       11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11., 11.,\n",
       "       11., 11., 11., 11., 11., 11.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45a8be1-ab6a-4a66-b781-7a0f6f86fabe",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### \"feature selection\"\n",
    "\n",
    "with \"feature selection\", selecting voxels that modulate statistically significantly according to object category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ec0f3e5-8b6f-4605-b387-1c1509d0838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"feature selection\" based on picking statistically significant voxels\n",
    "\n",
    "def feature_selection(train_pats, train_targs, test_pats):\n",
    "    pval = 0.1\n",
    "    \n",
    "    keep_these = np.zeros((train_pats.shape[1],))\n",
    "    \n",
    "    # loop through every voxel\n",
    "    for v in range(train_pats.shape[1]):\n",
    "        groups = []\n",
    "        # Loop through all 8 categories\n",
    "        for c in range(train_targets.shape[1]):\n",
    "            groups.append(train_pats[train_targs[:, c] == 1., v])\n",
    "           \n",
    "        # and statistically analyze it for category modulation\n",
    "        temp = sps.f_oneway(groups[0], groups[1], groups[2], groups[3],\n",
    "                            groups[4], groups[5], groups[6], groups[7])\n",
    "\n",
    "        if temp.pvalue < pval:\n",
    "            keep_these[v] = 1.\n",
    "    \n",
    "    keep_these = keep_these.astype(bool)\n",
    "    train_pats = train_pats[:, keep_these]\n",
    "    test_pats = test_pats[:, keep_these]\n",
    "    \n",
    "    return train_pats, test_pats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2fce4a-41af-4385-a528-9cec3726cc31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12763\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# an example \"leaving out\" a particular run\n",
    "example_test_run = 1\n",
    "\n",
    "train_these = runs != example_test_run\n",
    "test_these  = runs == example_test_run\n",
    "\n",
    "# an example of logical indexing\n",
    "train_patterns = patterns[train_these, :]\n",
    "train_targets = targets[train_these, :]\n",
    "\n",
    "test_patterns = patterns[test_these, :]\n",
    "test_targets = targets[test_these, :]\n",
    "\n",
    "# \"feature selection\" of statistically category-modulated voxels\n",
    "\n",
    "fs_train_patterns, fs_test_patterns = feature_selection(train_patterns,\n",
    "                                                        train_targets,\n",
    "                                                        test_patterns)\n",
    "\n",
    "# report some metrics on Haxby experiment (n_vox is for this fold)\n",
    "n_runs = np.max(runs).astype('int') + 1\n",
    "n_vox  = fs_train_patterns.shape[1]\n",
    "n_cats = targets.shape[1]\n",
    "\n",
    "cond_names = ['face', 'house', 'cat', 'shoe', 'scissors', 'bottle', \n",
    "              'scrambledpix', 'chair', 'rest']\n",
    "\n",
    "print(n_runs)\n",
    "print(n_vox)\n",
    "print(n_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ae2c30",
   "metadata": {},
   "source": [
    "# Q1a:\n",
    "> Leave-one-out cross validation\n",
    "\n",
    "Save the classification\tfor\teach of\tthe\t12 folds and note the classification performance on\teach of\t the 8\tcategories and\tfor\teach of\tthe\t5 iterations. First, report\tthe\taverage\tclassification accuracy overall. Next, examine and\treport\tthe\taverage\tclassification\tfor\teach object category. Are certain\tcategories classified more accurately?\tHow\tmuch variability is\tthere across categories\tand\tacross\tfolds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b57decde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_accuracy(history):\n",
    "    \"\"\"Accuracy Plotting Function\"\"\"\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.title('Model Accuracy As a Function of Number of Epochs Trained')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_model_loss(history):\n",
    "    \"\"\"Loss Plotting Function\"\"\"\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.title('Model Loss As a Function of Number of Epochs Trained')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "    \n",
    "# SET UP THE FOLD R         \n",
    "# DO FEATURE SELECTION FOR THIS FOLD\n",
    "def cross_fold_fs(example_test_run):\n",
    "    train_these = runs != example_test_run\n",
    "    test_these  = runs == example_test_run\n",
    "\n",
    "    # an example of logical indexing\n",
    "    train_patterns = patterns[train_these, :]\n",
    "    train_targets = targets[train_these, :]\n",
    "\n",
    "    test_patterns = patterns[test_these, :]\n",
    "    test_targets = targets[test_these, :]\n",
    "\n",
    "    # \"feature selection\" of statistically category-modulated voxels\n",
    "    fs_train_patterns, fs_test_patterns = feature_selection(train_patterns,\n",
    "                                                            train_targets,\n",
    "                                                            test_patterns)\n",
    "    \n",
    "    return fs_train_patterns, fs_test_patterns, test_targets\n",
    "\n",
    "# TRAIN LINEAR CLASSIFIER\n",
    "def make_train_model(nin, nout, fs_train_patterns, train_targets, my_epochs=30, my_batch=128):\n",
    "    network_softmax = models.Sequential()\n",
    "    network_softmax.add(layers.Dense(nout, \n",
    "                             activation='softmax',\n",
    "                             kernel_regularizer=l2(0.001),\n",
    "                             input_shape=(nin,)))\n",
    "\n",
    "    network_softmax.compile(optimizer='adam', \n",
    "                    loss='categorical_crossentropy', \n",
    "                    metrics=['accuracy', 'mse'])\n",
    "\n",
    "    history = network_softmax.fit(fs_train_patterns,\n",
    "                                  train_targets,\n",
    "                                  epochs=my_epochs,\n",
    "                                  batch_size=my_batch,\n",
    "                                  verbose=False)\n",
    "    \n",
    "    return network_softmax\n",
    "\n",
    "# TEST LINEAR CLASSIFIER\n",
    "def test_classifier(fs_test_patterns, n_cats, targets, network_softmax):\n",
    "# Get train, test arrays for each CATEGORY\n",
    "    n_test_dim, n_test_vox = fs_test_patterns.shape\n",
    "\n",
    "    i_test_prev = 0\n",
    "    test_dims = n_test_dim//n_cats\n",
    "    i_test_next = test_dims*(idx_cat+1)\n",
    "\n",
    "    test_patterns_array[idx_cat] = fs_test_patterns[i_test_prev:i_test_next, :]\n",
    "    i_test_prev, i_test_next = i_test_next, test_dims*(idx_cat+1)\n",
    "\n",
    "    # Take target array (LABELS) and build into a full matrix\n",
    "    target_array = targets[idx_cat, :]\n",
    "    for j in range(test_patterns_array[idx_cat].shape[0] - 1):\n",
    "        target_array = np.vstack((target_array,targets[idx_cat, :]))\n",
    "    test_targets_array[idx_cat] = target_array\n",
    "\n",
    "    # Now, evaluate based on those CATEGORIES\n",
    "    results = network_softmax.evaluate(test_patterns_array[idx_cat], \n",
    "                           test_targets_array[idx_cat], \n",
    "                           verbose=False)\n",
    "    #test_loss, test_acc = results[0], results[1]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a353530",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = 5\n",
    "\n",
    "n_cross_folds = 12\n",
    "n_cats = targets.shape[1]\n",
    "nout = n_cats\n",
    "\n",
    "test_patterns_array = [-1]*n_cats\n",
    "test_targets_array = [-1]*n_cats\n",
    "    \n",
    "classification_array = np.zeros((n_runs, n_cross_folds, n_cats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df9fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent this from running for now\n",
    "if 0 is True:\n",
    "    for run_num in range(n_runs):\n",
    "        for n_cross in range(n_cross_folds):\n",
    "            # SET UP THE FOLD R, DO FEATURE SELECTION FOR THIS FOLD\n",
    "            fs_train_patterns, fs_test_patterns, test_targets = cross_fold_fs(n_cross)\n",
    "\n",
    "            # Input dimensions is the number of voxels\n",
    "            nin  = fs_train_patterns.shape[1]\n",
    "\n",
    "            # TRAIN LINEAR CLASSIFIER\n",
    "            network_softmax = make_train_model(nin, nout, fs_train_patterns, train_targets)\n",
    "\n",
    "            for idx_cat in range(n_cats):\n",
    "                # TEST LINEAR CLASSIFIER\n",
    "                results = test_classifier(fs_test_patterns, n_cats, test_targets, network_softmax)\n",
    "\n",
    "                # SAVE CLASSIFIER PERFORMANCE\n",
    "                classification_array[run_num, n_cross, idx_cat] = results[1]\n",
    "\n",
    "# REPORT AVERAGE OF CLASSIFIERS\n",
    "# SAVE AVERAGE PERFORMANCE TO ARRAY TO CALCUALTE CHANCE RANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "495f1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_rows(source, target):\n",
    "    return np.where((source == target).all(axis=1))[0]\n",
    "\n",
    "# print(find_rows(test_targets, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10602d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kdmen\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (13589,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_37824/4069212783.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             results = network_softmax.evaluate(CAT_test_patterns, \n\u001b[0;32m     35\u001b[0m                                    \u001b[0mbase\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m                                    verbose=False)\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[1;31m#test_loss, test_acc = results[0], results[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mclassification_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrun_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_cross\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx_cat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'steps'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    942\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2649\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2650\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2651\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2652\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2653\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\NSCenv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    383\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    386\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (13589,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "my_epochs = 30\n",
    "my_batch = 128\n",
    "\n",
    "for run_num in range(n_runs):\n",
    "    for n_cross in range(n_cross_folds):\n",
    "        # Get each CROSS FOLD\n",
    "        fs_train_patterns, fs_test_patterns, test_targets = cross_fold_fs(n_cross)\n",
    "\n",
    "        # Input dimensions is the number of voxels\n",
    "        nin  = fs_train_patterns.shape[1]\n",
    "\n",
    "        network_softmax = models.Sequential()\n",
    "        network_softmax.add(layers.Dense(nout, \n",
    "                                 activation='softmax',\n",
    "                                 kernel_regularizer=l2(0.001),\n",
    "                                 input_shape=(nin,)))\n",
    "\n",
    "        network_softmax.compile(optimizer='adam', \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy', 'mse'])\n",
    "\n",
    "        history = network_softmax.fit(fs_train_patterns,\n",
    "                                      train_targets,\n",
    "                                      epochs=my_epochs,\n",
    "                                      batch_size=my_batch,\n",
    "                                      verbose=False)\n",
    "\n",
    "        for idx_cat in range(n_cats):\n",
    "            base = np.zeros(n_cats)\n",
    "            base[idx_cat] = 1\n",
    "            CAT_test_patterns = find_rows(test_targets, base)\n",
    "            \n",
    "            # Now, evaluate based on those CATEGORIES\n",
    "            results = network_softmax.evaluate(CAT_test_patterns, \n",
    "                                   base, \n",
    "                                   verbose=False)\n",
    "            #test_loss, test_acc = results[0], results[1]\n",
    "            classification_array[run_num, n_cross, idx_cat] = results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b0a59e",
   "metadata": {},
   "source": [
    "## LEGACY VERSION\n",
    "\n",
    "for run_num in range(n_runs):\n",
    "    for n_cross in range(n_cross_folds):\n",
    "        # Get each CROSS FOLD\n",
    "        fs_train_patterns, fs_test_patterns = cross_fold_fs(n_cross)\n",
    "\n",
    "        # Input dimensions is the number of voxels\n",
    "        nin  = fs_train_patterns.shape[1]\n",
    "        \n",
    "\n",
    "        network_softmax = models.Sequential()\n",
    "        network_softmax.add(layers.Dense(nout, \n",
    "                                 activation='softmax',\n",
    "                                 kernel_regularizer=l2(0.001),\n",
    "                                 input_shape=(nin,)))\n",
    "\n",
    "        network_softmax.compile(optimizer='adam', \n",
    "                        loss='categorical_crossentropy', \n",
    "                        metrics=['accuracy', 'mse'])\n",
    "\n",
    "        history = network_softmax.fit(fs_train_patterns,\n",
    "                                      train_targets,\n",
    "                                      epochs=my_epochs,\n",
    "                                      batch_size=my_batch,\n",
    "                                      verbose=False)\n",
    "\n",
    "        for idx_cat in range(n_cats):\n",
    "            # Get train, test arrays for each CATEGORY\n",
    "            n_train_dim, n_train_vox = fs_train_patterns.shape\n",
    "            n_test_dim, n_test_vox = fs_test_patterns.shape\n",
    "\n",
    "            i_train_prev, i_test_prev = 0, 0\n",
    "            train_dims, test_dims = n_train_dim//n_cats, n_test_dim//n_cats\n",
    "            i_train_next, i_test_next = train_dims*(idx_cat+1), test_dims*(idx_cat+1)\n",
    "\n",
    "            test_patterns_array[idx_cat] = fs_test_patterns[i_test_prev:i_test_next, :]\n",
    "            i_test_prev, i_test_next = i_test_next, test_dims*(idx_cat+1)\n",
    "\n",
    "            # Take target array (LABELS) and build into a full matrix\n",
    "            target_array = targets[idx_cat, :]\n",
    "            for j in range(test_patterns_array[idx_cat].shape[0] - 1):\n",
    "                target_array = np.vstack((target_array,targets[idx_cat, :]))\n",
    "            test_targets_array[idx_cat] = target_array\n",
    "            \n",
    "            # Now, evaluate based on those CATEGORIES\n",
    "            results = network_softmax.evaluate(test_patterns_array[idx_cat], \n",
    "                                   test_targets_array[idx_cat], \n",
    "                                   verbose=False)\n",
    "            #test_loss, test_acc = results[0], results[1]\n",
    "            classification_array[run_num, n_cross, idx_cat] = results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "oaca = np.sum(classification_array)/classification_array.size\n",
    "print(f\"The overall average classification accuracy is {oaca:.4}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23392c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_accs = np.zeros((n_runs, n_cats))\n",
    "\n",
    "for i in range(n_runs):\n",
    "    for k in range(n_cats):\n",
    "        old_sum = cat_accs[i, k]\n",
    "        new_sum = np.sum(classification_array[i, :, k])\n",
    "        running_sum = old_sum + new_sum\n",
    "        cat_accs[i, k] = running_sum\n",
    "        \n",
    "cat_accs_list = []\n",
    "\n",
    "for i in range(n_cats):\n",
    "    cat_accs_list.append(np.sum(cat_accs[:, i]))\n",
    "    \n",
    "cat_accs_final = [acc/(classification_array.size/n_cats) for acc in cat_accs_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79655626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_accs_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7de1032",
   "metadata": {},
   "source": [
    "Are\tcertain\tcategories classified more accurately? How\tmuch variability is there across categories\tand\tacross\tfolds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fdc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_loss(history)\n",
    "plot_model_accuracy(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977678a-e405-4651-ba68-db4770a1be8b",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "#### function to scramble targets for permutation test\n",
    "\n",
    "(as described in class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5771093",
   "metadata": {},
   "source": [
    "what if we were to scramble the labels associated\n",
    "with different brain scans (don't touch brain scans)?\n",
    "e.g., on one run, voxel measures when a face is shown\n",
    "would be labeled as being trials when a chair was shown,\n",
    "and on another run, voxel measures when a face is shown\n",
    "would be labeled as being trials when a shoe was shown\n",
    "the performance for a classifier trained on that random\n",
    "shuffling of labels would indicate the level of performance\n",
    "you would expect for a classifier \"by chance\"\n",
    "\n",
    "PERMUTATE THE LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea2870-1fbd-49d1-9d71-e02e85d728fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to scramble the category labels for a permutation analysis\n",
    "\n",
    "def scramble_targets(targets, runs):\n",
    "    # there are 12 runs\n",
    "    n_runs = np.max(runs).astype('int') + 1\n",
    "    \n",
    "    # there are 8 categories\n",
    "    n_cats = targets.shape[1]\n",
    "    \n",
    "    # this will contain the scrambled category labels\n",
    "    scram_targets = np.zeros(targets.shape)\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        # first find the category labels just for this run\n",
    "        these_targets = targets[runs==i, :].copy()\n",
    "        \n",
    "        # this shuffles the columns of these_targets, which preserves \n",
    "        # the block structure of the experiment\n",
    "        scram_targets_this_run = these_targets[:, np.random.permutation(n_cats)]\n",
    "        \n",
    "        # this copies the scrambled targets for this run into the \n",
    "        # appropriate rows of the scrambled category labels matrix\n",
    "        scram_targets[runs==i,:] = scram_targets_this_run\n",
    "\n",
    "    return scram_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf4a6a1-bb8c-4fa1-8779-6f65c09f5a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_perms = 10\n",
    "\n",
    "for p in range(n_perms):\n",
    "    # PERMUTE LABELS\n",
    "    scram_targets = scramble_targets(targets, runs)\n",
    "    \n",
    "    # SET UP DATA STRUCTURES FOR STORAGE\n",
    "    for i in range(n_iters):\n",
    "        for r in range(n_runs):\n",
    "            # SET UP THE FOLD R\n",
    "            \n",
    "            # DO FEATURE SELECTION FOR THIS FOLD\n",
    "            fs_train_patterns, fs_test_patterns = feature_selection(train_patterns,\n",
    "                                                        scram_train_targets,\n",
    "                                                        test_patterns)\n",
    "            \n",
    "            # TRAIN LINEAR CLASSIFIER\n",
    "            # TEST LINEAR CLASSIFIER\n",
    "            # SAVE CLASSIFIER PERFORMANCE\n",
    "    # REPORT AVERAGE OF CLASSIFIERS\n",
    "    # SAVE AVERAGE PERFORMANCE TO ARRAY TO CALCUALTE CHANCE RANGE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
